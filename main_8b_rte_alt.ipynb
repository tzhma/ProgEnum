{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "run with ox (for higher version of networkx to plot self-loop)\n",
    "df, df_enumP, mergP_dict has to be globally accessible\n",
    "laptop: 25m\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from itertools import permutations, combinations, product, islice\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import ChainMap\n",
    "import multiprocess\n",
    "from core_enum import fl, reshape_prog, gen_stdz_prog\n",
    "from core_pte import gen_perm_prog_array, gen_edit_distance_list, gen_one_merger_prog_list\n",
    "\n",
    "num_cpus = int(multiprocess.cpu_count()*1)\n",
    "print('num_cpus = %s'%num_cpus)\n",
    "\n",
    "\n",
    "## parameters\n",
    "extension = ('5108', '268536')[1]\n",
    "progsize_max = (4,5)[1]\n",
    "filename_load = 'df_pte_' + extension + '_alt'\n",
    "filename_save = 'df_rpte_' + extension + '_alt'\n",
    "df_enumP = pd.read_pickle('data/df_enumP_para14_alt') ###\n",
    "\n",
    "## load\n",
    "try: mergP_dict = pickle.load(open('data/mergP_dict','rb'))\n",
    "except FileNotFoundError: mergP_dict = {}\n",
    "df = pd.read_pickle('data/%s'%filename_load)\n",
    "# df.index = df['target'].values ####\n",
    "roots = df[df['progsize']==2].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions: prep for rPTE with multiprocess\n",
    "def gen_branch_partbranch(prog_id):\n",
    "\tprog_id, prog_id_0 = df[df['target']==prog_id][['target', 'source']].values[0]\n",
    "\n",
    "\t# append root directly\n",
    "\tif prog_id in roots:\n",
    "\t\tbranch = (prog_id,)\n",
    "\t\tpartbranch = (prog_id,)\n",
    "\t\treturn branch, partbranch\n",
    "\n",
    "\t# append prog_id into branch\n",
    "\tbranch = [prog_id, prog_id_0]\n",
    "\tprog_id_prev = prog_id_0\n",
    "\twhile prog_id_prev not in roots:\n",
    "\t\tprog_id_curr = branch[-1]\n",
    "\t\tprog_id_prev = df[df['target']==prog_id_curr]['source'].values[0]\n",
    "\t\tif prog_id_prev not in branch: \n",
    "\t\t\tbranch.append(prog_id_prev)\n",
    "\t\telse: \n",
    "\t\t\troots.append(prog_id_prev)\n",
    "\t\t\tbreak\n",
    "\t\t# branch.append(prog_id_prev)\n",
    "\tbranch = tuple(branch)\n",
    "\n",
    "\t# convert prog_id into part\n",
    "\tpartbranch = []\n",
    "\tfor prog_id in branch[:-1]:\n",
    "\t\tpart = df[df['target']==prog_id]['part'].values[0]\n",
    "\t\tpartbranch.append(''.join([str(x) for x in part]))\n",
    "\tpartbranch.append(branch[-1])\n",
    "\tpartbranch = tuple(partbranch)\n",
    "\treturn branch, partbranch\n",
    "\n",
    "def gen_ecc(prog_id):\n",
    "\t# if df.loc[prog_id, 'if_leaf']==True: ec=0\n",
    "\tif df[df['target']==prog_id]['if_leaf'].values[0]: ec=0\n",
    "\telse:\n",
    "\t\toutward_branches = []\n",
    "\t\tfor branch in df['branch'].values:\n",
    "\t\t\ttry: outward_branches.append(branch[branch.index(prog_id)+1:])\n",
    "\t\t\texcept ValueError: continue\n",
    "\t\tec = max([len(x) for x in outward_branches])\n",
    "\treturn ec\n",
    "\n",
    "def gen_df_branch_partbranch():\n",
    "\t'''\n",
    "\tmultiprocess: reduce tree via grouping nodes according to partitions\n",
    "\t'''\n",
    "\tprog_id_list = df['target'].values\n",
    "\tbranch_list, partbranch_list = [], []\n",
    "\twith multiprocess.Pool(num_cpus) as p:\n",
    "\t\tjobs = p.map(gen_branch_partbranch, prog_id_list)\n",
    "\t\tfor prog_id, job in zip(prog_id_list, jobs):\n",
    "\t\t\tbranch_list.append(job[0])\n",
    "\t\t\tpartbranch_list.append(job[1])\n",
    "\t\t\tif prog_id % max(prog_id_list[-1]//100,1)==0: print(prog_id, job)\n",
    "\n",
    "\t# update df\n",
    "\tdf['branch'] = [x[::-1] for x in branch_list] # from root to leaf\n",
    "\tdf['partbranch'] = [x[::-1] for x in partbranch_list]\n",
    "\treturn\n",
    "\n",
    "def gen_df_with_if_leaf():\n",
    "\t'''\n",
    "\tappend new column 'if_leaf' in df\n",
    "\t'''\n",
    "\tnonleafnodes = sorted(set(fl([list(x[:-1]) for x in df['branch'].values])))\n",
    "\tdf['if_leaf'] = [i not in nonleafnodes for i in df['target'].values]\n",
    "\treturn\n",
    "\n",
    "def gen_df_ecc():\n",
    "\t'''\n",
    "\tdf needs to have 'if_leaf' & 'branch'\n",
    "\t'''\n",
    "\tprog_id_list = df['target'].values\n",
    "\tecc_list = []\n",
    "\twith multiprocess.Pool(num_cpus) as p:\n",
    "\t\tjobs = p.map(gen_ecc, prog_id_list)\n",
    "\t\tfor prog_id, job in zip(prog_id_list, jobs):\n",
    "\t\t\tecc_list.append(job)\n",
    "\t\t\tif prog_id % max(prog_id_list[-1]//10,1)==0: print(prog_id, job)\n",
    "\n",
    "\t# update df\n",
    "\tdf['ecc'] = ecc_list\n",
    "\treturn\n",
    "\n",
    "def gen_df_tarP_souP():\n",
    "\t'''\n",
    "\tmaybe use only in standard embedding rather than the reward-shuffled ones\n",
    "\t'''\n",
    "\t# add first program\n",
    "\tprog0 = df_enumP.loc[df.iloc[0]['id_unique'], 'program']\n",
    "\ttar0 = df['target'][0]\n",
    "\ttar0_uni = df[df['target']==tar0]['id_unique'].values[0]\n",
    "\ttarP_dict = {tar0:prog0}\n",
    "\ttarP_list, souP_list = [prog0], [prog0] # first program has no source\n",
    "\n",
    "\t# append lineage\n",
    "\toutmap0, inmap0 = prog0\n",
    "\tprog0_lin = (np.ones_like(outmap0)*tar0_uni, np.ones_like(inmap0)*tar0_uni)\n",
    "\ttarP_lin_dict = {tar0:prog0_lin}\n",
    "\ttarP_lin_list, souP_lin_list = [prog0_lin], [prog0_lin]\n",
    "\n",
    "\tfor tar in df['target'].values[1:]:\n",
    "\t\tif tar % max(len(df)//10,1)==0: print(tar)\n",
    "\t\t# load\n",
    "\t\ttar_uni, sou, part = df[df['target']==tar][['id_unique', 'source', 'part']].values[0]\n",
    "\t\tsou_uni = df[df['target']==sou]['id_unique'].values[0]\n",
    "\t\tsouP = tarP_dict[sou]\n",
    "\t\tsouP_lin = tarP_lin_dict[sou]\n",
    "\t\ttarP = df_enumP.loc[tar_uni, 'program']\n",
    "\n",
    "\t\t# get minP for both sou and tar\n",
    "\t\ttar_permP_arr = gen_perm_prog_array(tarP)\n",
    "\t\ttry: sou_mergP_list = mergP_dict[(sou_uni, part)] # sou_mergP has stdz outmap\n",
    "\t\texcept KeyError:\n",
    "\t\t\t# sou_mergP_list = [souP + (tuple(range(len(souP[0]))),)]\n",
    "\t\t\tsou_mergP_list = gen_one_merger_prog_list(souP, part)\n",
    "\t\t\tif sum(part)!=len(souP[0]): mergP_dict[(sou_uni, part)] = sou_mergP_list\n",
    "\n",
    "\t\t# sou_mergP_list = gen_one_merger_prog_list(souP, part)\n",
    "\t\t# if sum(part)!=len(souP[0]): mergP_dict[(sou_uni, part)] = sou_mergP_list\n",
    "\n",
    "\t\td_list, min_permP_list = gen_edit_distance_list(tar_permP_arr, sou_mergP_list)\n",
    "\t\t# print(min(d_list))\n",
    "\t\tidx_min = np.argmin(d_list)\n",
    "\t\tmin_tarP = min_permP_list[idx_min]\n",
    "\t\tmin_souP = sou_mergP_list[idx_min][:2] # 3rd index is linmap\n",
    "\t\tmin_tarP = (np.array(min_tarP[0]), np.reshape(min_tarP[1], [len(min_tarP[0]), 2]))\n",
    "\t\tmin_souP = (np.array(min_souP[0]), np.reshape(min_souP[1], [len(min_souP[0]), 2]))\n",
    "\n",
    "\t\t# compute lineage\n",
    "\t\tlinmap = sou_mergP_list[idx_min][-1]\n",
    "\t\tsou_mergP_lin = np.array([souP_lin[0][x] for x in linmap]), np.array([souP_lin[1][x].tolist() for x in linmap]) # expend souP_lin into its mergP_lin\n",
    "\t\toutmap_lin = (min_tarP[0]==min_souP[0])*sou_mergP_lin[0] + (min_tarP[0]!=min_souP[0])*tar_uni\n",
    "\t\tinmap_lin = (min_tarP[1]==min_souP[1])*sou_mergP_lin[1] + (min_tarP[1]!=min_souP[1])*tar_uni\n",
    "\t\ttarP_lin = (outmap_lin, inmap_lin)\n",
    "\n",
    "\t\t# append\n",
    "\t\ttarP_dict[tar] = min_tarP\n",
    "\t\ttarP_lin_dict[tar] = tarP_lin\n",
    "\t\ttarP_list.append(min_tarP)\n",
    "\t\tsouP_list.append(min_souP)\n",
    "\t\ttarP_lin_list.append(tarP_lin)\n",
    "\t\tsouP_lin_list.append(sou_mergP_lin)\n",
    "\n",
    "\t# update df\n",
    "\tdf['tarP'] = tarP_list\n",
    "\tdf['souP'] = souP_list\n",
    "\tdf['tarP_lin'] = tarP_lin_list\n",
    "\tdf['souP_lin'] = souP_lin_list\n",
    "\n",
    "\tnum_mut_list = []\n",
    "\tfor tarP_lin in df['tarP_lin'].values:\n",
    "\t\tnum_mut = (tarP_lin[0]!=4).sum() + (tarP_lin[1]!=4).sum()\n",
    "\t\tnum_mut_list.append(num_mut)\n",
    "\tdf['num_mut'] = num_mut_list\n",
    "\treturn\n",
    "\n",
    "\n",
    "## functions: rPTE\n",
    "def gen_part_and_branch_dict():\n",
    "\t'''\n",
    "\ta unique branch is a unique sequence of partition + eccentricity with consistent souP\n",
    "\t'''\n",
    "\tprint('gen_part_and_branch_dict...')\n",
    "\n",
    "\t# part id dict is simple\n",
    "\tpart_id_dict = {x:i for i,x in enumerate(sorted(set(df['part'].values)))}\n",
    "\n",
    "\t# initialize for branch_dicts\n",
    "\tbranch_proglist_dict = {x+(0,):[] for x in sorted(set([tuple(x) for x in df[['partbranch', 'ecc']].values]))}\n",
    "\tbranch_source_dict = {x+(0,):() for x in sorted(set([tuple(x) for x in df[['partbranch', 'ecc']].values]))}\n",
    "\n",
    "\t# append first program\n",
    "\ttar, sou, ptb, ecc = df[['target', 'source', 'partbranch', 'ecc']].values[0]\n",
    "\tsou_ptb, sou_ecc = df[df['target']==sou][['partbranch', 'ecc']].values[0]\n",
    "\tbranch_proglist_dict[(ptb, ecc, 0)].append(tar)\n",
    "\tbranch_source_dict[(ptb, ecc, 0)] = (sou_ptb, sou_ecc, 0)\n",
    "\n",
    "\t# append prog_id\n",
    "\tfor tar, sou, ptb, ecc in df[['target', 'source', 'partbranch', 'ecc']].values[1:]:\n",
    "\t\tsou_ptb, sou_ecc = df[df['target']==sou][['partbranch', 'ecc']].values[0]\n",
    "\n",
    "\t\t# find souP's branch\n",
    "\t\tif_found = False\n",
    "\t\tfor i in range(100):\n",
    "\t\t\tsou_ptb_ext = (sou_ptb, sou_ecc, i)\n",
    "\t\t\ttry: \n",
    "\t\t\t\tbranch_proglist_dict[sou_ptb_ext]\n",
    "\t\t\t\tif sou in branch_proglist_dict[sou_ptb_ext]: \n",
    "\t\t\t\t\tif_found = True\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\texcept Exception: continue ####\n",
    "\t\tif not if_found: sou_ptb_ext = (sou_ptb, sou_ecc, 0)\n",
    "\n",
    "\t\t# assign tarP's branch\n",
    "\t\tfor i in range(10000):\n",
    "\t\t\tif i%100==13: print(tar, i)\n",
    "\t\t\ttar_ptb_ext = (ptb, ecc, i)\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\tsou_ptb_ext_0 = branch_source_dict[tar_ptb_ext]\n",
    "\t\t\t\tif len(sou_ptb_ext_0)>0: # having source means tar_ptb_ext is registered\n",
    "\t\t\t\t\tif sou_ptb_ext_0==sou_ptb_ext: # same source means current tar_ptb_ext is consist\n",
    "\t\t\t\t\t\tbranch_proglist_dict[tar_ptb_ext].append(tar)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\telse: # empty slot to be used\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tbranch_proglist_dict[tar_ptb_ext].append(tar)\n",
    "\t\t\t\t\t\tbranch_source_dict[tar_ptb_ext] = sou_ptb_ext\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t\tbranch_proglist_dict[tar_ptb_ext] = []\n",
    "\t\t\t\t\t\tbranch_proglist_dict[tar_ptb_ext].append(tar)\n",
    "\t\t\t\t\t\tbranch_source_dict[tar_ptb_ext] = sou_ptb_ext\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\texcept KeyError: # tar_ptb_ext not registered yet\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tbranch_proglist_dict[tar_ptb_ext].append(tar)\n",
    "\t\t\t\t\tbranch_source_dict[tar_ptb_ext] = sou_ptb_ext\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\tbranch_proglist_dict[tar_ptb_ext] = []\n",
    "\t\t\t\t\tbranch_proglist_dict[tar_ptb_ext].append(tar)\n",
    "\t\t\t\t\tbranch_source_dict[tar_ptb_ext] = sou_ptb_ext\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t# sort keys & append\n",
    "\tkey_sorted = sorted(branch_source_dict)\n",
    "\tbranch_source_dict = {x:branch_source_dict[x] for x in key_sorted}\n",
    "\tbranch_proglist_dict = {x:branch_proglist_dict[x] for x in key_sorted}\n",
    "\tbranch_id_dict = {x:i for i,x in enumerate(key_sorted)}\n",
    "\tbranch_tar_sou_dict = {branch_id_dict[x]:branch_id_dict[branch_source_dict[x]] for x in key_sorted}\n",
    "\tbranch_tar_proglist_dict = {branch_id_dict[x]:branch_proglist_dict[x] for x in key_sorted}\n",
    "\n",
    "\t# create prog_id_branch_id_dict\n",
    "\tprog_id_branch_id_dict = {}\n",
    "\tfor branch_id, proglist in branch_tar_proglist_dict.items():\n",
    "\t\tfor prog_id in proglist:\n",
    "\t\t\tprog_id_branch_id_dict[prog_id] = branch_id\n",
    "\tprog_id_branch_id_dict = {x:prog_id_branch_id_dict[x] for x in sorted(prog_id_branch_id_dict)}\n",
    "\n",
    "\treturn part_id_dict, branch_id_dict, branch_tar_sou_dict, branch_tar_proglist_dict, prog_id_branch_id_dict\n",
    "\n",
    "def gen_df_rpte():\n",
    "\t'''\n",
    "\tgenerate:\n",
    "\t1) df_rpte (index: branch_id)\n",
    "\t2) df_rpte_part_id_dict\n",
    "\t3) df_rpte_branch_id_dict\n",
    "\t'''\n",
    "\t# dict for mapping prog_id to branch_id\n",
    "\tbranch_dicts = gen_part_and_branch_dict()\n",
    "\tpart_id_dict, branch_id_dict, branch_tar_sou_dict, branch_tar_proglist_dict, prog_id_branch_id_dict = branch_dicts\n",
    "\n",
    "\t# collect branches to df\n",
    "\tquery = []\n",
    "\tprint(len(branch_id_dict), 'partbranches to be process...')\n",
    "\tfor (partbranch, ecc, ext), tar in branch_id_dict.items():\n",
    "\t\tprint(partbranch)\n",
    "\n",
    "\t\tif len(partbranch)==1: part_id = 0\n",
    "\t\telse: part_id = part_id_dict[tuple([int(x) for x in partbranch[-1]])]\n",
    "\n",
    "\t\tsou = branch_tar_sou_dict[tar]\n",
    "\t\tprog_ids = branch_tar_proglist_dict[tar]\n",
    "\n",
    "\t\t# read other prog attr\n",
    "\t\tunique_ids, progsizes, eRs = np.array([df[df['target']==x][['id_unique','progsize','eR']].values[0] for x in prog_ids]).T\n",
    "\t\tunique_ids = unique_ids.astype('int')\n",
    "\t\tprogsize = progsizes.mean().astype('int')\n",
    "\t\teR_mean, eR_std = eRs.mean(), eRs.std()\n",
    "\t\tnum_prog = len(prog_ids)\n",
    "\t\tdeg = len(partbranch)-1\n",
    "\n",
    "\t\t# append\n",
    "\t\tquery.append([tar, sou, tar, part_id, unique_ids, prog_ids, eRs, progsize, eR_mean, eR_std, num_prog, deg, ecc])\n",
    "\n",
    "\t# df\n",
    "\tdf_rpte = pd.DataFrame(query, columns=['id', 'source', 'target', 'part_id', 'unique_ids', 'prog_ids', 'eRs', 'progsize', 'eR_mean', 'eR_std', 'num_prog', 'deg', 'ecc'])\n",
    "\n",
    "\t# dict_r = {v:''.join([str(x) for x in k]) for (k,v) in part_id_dict.items()}\n",
    "\t# pd.DataFrame.from_dict(dict_r, orient='index', columns=['part']).to_csv('gephi/%s_part_dict.csv'%filename_save, index=False)\n",
    "\n",
    "\t# df_rpte.to_csv('gephi/%s.csv'%filename_save, index=False)\n",
    "\t# dict_r = {v:k for (k,v) in branch_id_dict.items()}\n",
    "\t# pd.DataFrame.from_dict(dict_r, orient='index').to_csv('gephi/%s_branch_dict.csv'%filename_save, index=False)\n",
    "\n",
    "\treturn df_rpte, part_id_dict, branch_id_dict, branch_tar_sou_dict, branch_tar_proglist_dict, prog_id_branch_id_dict\n",
    "\n",
    "def gen_rpte_branch(tar, roots=[0]):\n",
    "\ttar, sou = df_rpte[df_rpte['target']==tar][['target', 'source']].values[0]\n",
    "\tif tar in roots:\n",
    "\t\tbranch = (tar,)\n",
    "\t\treturn branch\n",
    "\n",
    "\t# append prog_id into branch\n",
    "\tbranch = [tar, sou]\n",
    "\ttar_prev = sou\n",
    "\twhile tar_prev not in roots:\n",
    "\t\ttar_curr = branch[-1]\n",
    "\t\ttar_prev = df_rpte[df_rpte['target']==tar_curr]['source'].values[0]\n",
    "\t\tbranch.append(tar_prev)\n",
    "\tbranch = tuple(branch)\n",
    "\treturn branch\n",
    "\n",
    "def gen_df_rpte_branch():\n",
    "\t'''\n",
    "\teff_rpte is a reward filtered subtree\n",
    "\t'''\n",
    "\ttar_list = df_rpte['target'].values\n",
    "\tbranch_list = []\n",
    "\tfor tar in tar_list:\n",
    "\t\tbranch = gen_rpte_branch(tar)\n",
    "\t\tbranch_list.append(branch)\n",
    "\n",
    "\t# update df & save\n",
    "\tdf_rpte['branch'] = [x[::-1] for x in branch_list] # from root to leaf\n",
    "\treturn\n",
    "\n",
    "\n",
    "## functions: good program network (GPN)\n",
    "def gen_df_rpte_good_prog_net(eR_thres=0.277911, progsize_max=5):\n",
    "\t'''\n",
    "\tnote: run this after loading:\n",
    "\tpart_id_dict, branch_id_dict, branch_tar_sou_dict, branch_tar_proglist_dict, prog_id_branch_id_dict = pickle.load(open('data/df_rpte_%s_branch_dicts'%extension,'rb'))\n",
    "\n",
    "\ta single-component good program network extracted from df_rpte\n",
    "\tdf: e.g., df_rpte_5015_branch\n",
    "\tdf_rpte: e.g., df_rpte_5015\n",
    "\t'''\n",
    "\t# set df progsize upperbound\n",
    "\tdff = df[df['progsize']<=progsize_max]\n",
    "\n",
    "\t# find GPN\n",
    "\ttar_list = dff[dff['eR']>=eR_thres]['target'].values\n",
    "\tidx_list = dff[dff['eR']>=eR_thres].index\n",
    "\ttar_all_list = sorted(set(fl(dff.loc[idx_list, 'branch'])))\n",
    "\ttar_conn_list = [x for x in tar_all_list if x not in tar_list]\n",
    "\teR_all_list = [dff[dff['target']==x]['eR'].values[0] for x in tar_all_list]\n",
    "\n",
    "\t# plot: eR_hist\n",
    "\tplt.figure(figsize=(10,4), dpi=300)\n",
    "\tplt.title(np.sort(eR_all_list)[:10])\n",
    "\tplt.hist(eR_all_list, bins=100)\n",
    "\tif not eR_thres==0.277911: plt.axvline(x=eR_thres, color='r', linestyle='dashed')\n",
    "\tplt.axvline(x=0.277911, color='r', linestyle='dashed')\n",
    "\tplt.axvline(x=0.263867, color='r', linestyle='dashed')\n",
    "\tplt.axvline(x=0.25, color='r', linestyle='dashed')\n",
    "\tplt.xlabel('eR')\n",
    "\tplt.show()\n",
    "\n",
    "\t# next: which compound nodes do they belong?\n",
    "\tcompound_list = [prog_id_branch_id_dict[x] for x in tar_list]\n",
    "\tcompound_all_list = [prog_id_branch_id_dict[x] for x in tar_all_list]\n",
    "\tcompound_conn_list = [prog_id_branch_id_dict[x] for x in tar_conn_list]\n",
    "\tcompound_set = sorted(set(compound_list))\n",
    "\tcompound_all_set = sorted(set(compound_all_list))\n",
    "\tcompound_conn_set = sorted(set(compound_conn_list))\n",
    "\n",
    "\tcount_dict = {x:compound_list.count(x) for x in compound_set}\n",
    "\tcount_all_dict = {x:compound_all_list.count(x) for x in compound_all_set}\n",
    "\tcount_conn_dict = {x:compound_conn_list.count(x) for x in compound_conn_set}\n",
    "\n",
    "\t# add other GPN attr in df_rpte\n",
    "\tGPN_prog_ids_list, GPN_eRs_list = [], []\n",
    "\tGPN_eR_mean_list,   GPN_eR_std_list = [], []\n",
    "\tGPN_num_prog_list = []\n",
    "\tGPN_conn_prog_ids_list = []\n",
    "\tfor compound_tar in df_rpte['target'].values:\n",
    "\t\tGPN_prog_ids = [x for x in df_rpte[df_rpte['target']==compound_tar]['prog_ids'].values[0] if x in tar_all_list]\n",
    "\t\tGPN_conn_prog_ids = [x for x in df_rpte[df_rpte['target']==compound_tar]['prog_ids'].values[0] if x in tar_conn_list]\n",
    "\t\tGPN_eRs = [dff[dff['target']==x]['eR'].values[0] for x in GPN_prog_ids]\n",
    "\t\tif len(GPN_eRs)==0:\n",
    "\t\t\tGPN_eR_mean = 0\n",
    "\t\t\tGPN_eR_std = 0\n",
    "\t\t\tGPN_num_prog = 0\n",
    "\t\telse:\n",
    "\t\t\tGPN_eR_mean, GPN_eR_std = np.mean(GPN_eRs), np.std(GPN_eRs)\n",
    "\t\t\tGPN_num_prog = len(GPN_prog_ids)\n",
    "\n",
    "\t\t# append\n",
    "\t\tGPN_prog_ids_list.append(GPN_prog_ids)\n",
    "\t\tGPN_conn_prog_ids_list.append(GPN_conn_prog_ids)\n",
    "\t\tGPN_eRs_list.append(GPN_eRs)\n",
    "\t\tGPN_eR_mean_list.append(GPN_eR_mean)\n",
    "\t\tGPN_eR_std_list.append(GPN_eR_std)\n",
    "\t\tGPN_num_prog_list.append(GPN_num_prog)\n",
    "\t#\n",
    "\tdf_rpte['if_GPN'] = [(x in compound_all_set)*1 for x in df_rpte['target'].values]\n",
    "\tdf_rpte['GPN_prog_ids'] = GPN_prog_ids_list\n",
    "\tdf_rpte['GPN_eRs'] = GPN_eRs_list\n",
    "\tdf_rpte['GPN_eR_mean'] = GPN_eR_mean_list\n",
    "\tdf_rpte['GPN_eR_std'] = GPN_eR_std_list\n",
    "\tdf_rpte['GPN_num_prog'] = GPN_num_prog_list\n",
    "\tdf_rpte['if_GPN_conn'] = [(x in compound_conn_set)*1 for x in df_rpte['target'].values]\n",
    "\tdf_rpte['GPN_conn_prog_ids'] = GPN_conn_prog_ids_list\n",
    "\n",
    "\t# extract GPN data\n",
    "\tdff_rpte = df_rpte[[x in compound_all_set for x in df_rpte['target'].values]]\n",
    "\tbranch_all_list = sorted(dff_rpte['branch'].values)\n",
    "\tlineage_list = []\n",
    "\tfor branch_0, branch in zip(branch_all_list[:-1], branch_all_list[1:]):\n",
    "\t\tif branch[:-1]!=branch_0: lineage_list.append(branch_0)\n",
    "\n",
    "\t# print\n",
    "\tprint('%s programs are needed for covering %s programs with eR>=eR_WSLG'%(len(tar_all_list), len(tar_list)))\n",
    "\tprint('%s compound nodes are needed for covering %s compound nodes with eR>=eR_WSLG'%(len(compound_all_set), len(compound_set)))\n",
    "\tprint('lineage:')\n",
    "\tprint(lineage_list)\n",
    "\n",
    "\treturn lineage_list, branch_all_list\n",
    "\n",
    "def gen_df_rpte_d2enumDB():\n",
    "\tdff = df.copy()\n",
    "\tdff.index = dff['target'].values\n",
    "\n",
    "\t# df d2enumDB\n",
    "\td2enumDB_list = []\n",
    "\tGPN_d2enumDB_list = []\n",
    "\tfor progids, GPN_progids in df_rpte[['prog_ids','GPN_prog_ids']].values:\n",
    "\t\t#\n",
    "\t\td2enumDBs = dff.loc[progids, 'd2enumDB'].values\n",
    "\t\tif d2enumDBs.size==0: d2enumDB = 0\n",
    "\t\telse: d2enumDB = np.around(d2enumDBs.mean(), 6)\n",
    "\t\td2enumDB_list.append(d2enumDB)\n",
    "\t\t#\n",
    "\t\tGPN_d2enumDBs = dff.loc[GPN_progids, 'd2enumDB'].values\n",
    "\t\tif GPN_d2enumDBs.size==0: GPN_d2enumDB = 0\n",
    "\t\telse: GPN_d2enumDB = np.around(GPN_d2enumDBs.mean(), 6)\n",
    "\t\tGPN_d2enumDB_list.append(GPN_d2enumDB)\n",
    "\t#\n",
    "\tdf_rpte['d2enumDB'] = d2enumDB_list\n",
    "\tdf_rpte['GPN_d2enumDB'] = GPN_d2enumDB_list\n",
    "\n",
    "\t# df d2_enumDB_branches\n",
    "\td2enumDB_branch_list = []\n",
    "\tGPN_d2enumDB_branch_list = []\n",
    "\tGPN_d2enumDB_branch_list_4 = []\n",
    "\tfor compound_id in df_rpte.index:\n",
    "\t\td2enumDB_branch = df_rpte.loc[list(df_rpte.loc[compound_id, 'branch']), 'd2enumDB'].values.tolist()\n",
    "\t\td2enumDB_branch_list.append(d2enumDB_branch)\n",
    "\t\tGPN_d2enumDB_branch = df_rpte.loc[list(df_rpte.loc[compound_id, 'branch']), 'GPN_d2enumDB'].values.tolist()\n",
    "\t\tGPN_d2enumDB_branch_list.append(GPN_d2enumDB_branch)\n",
    "\t#\n",
    "\tdf_rpte['d2enumDB_branch'] = d2enumDB_branch_list\n",
    "\tdf_rpte['GPN_d2enumDB_branch'] = GPN_d2enumDB_branch_list\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%% run: rPTE (laptop: 16m)\n",
    "# append new column to df\n",
    "gen_df_branch_partbranch()\n",
    "gen_df_with_if_leaf()\n",
    "gen_df_ecc()\n",
    "\n",
    "# collecting branches and reduce tree (laptop: 10m)\n",
    "df_rpte, part_id_dict, branch_id_dict, branch_tar_sou_dict, branch_tar_proglist_dict, prog_id_branch_id_dict = gen_df_rpte()\n",
    "\n",
    "# pickle\n",
    "if not os.path.exists('data'): os.makedirs('data')\n",
    "if not os.path.exists('gephi'): os.makedirs('gephi')\n",
    "\n",
    "df.to_pickle('data/%s_branch'%filename_load)\n",
    "df_rpte.to_pickle('data/%s'%filename_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
